"""
API Parcel Data Ingestion Lambda Function

This Lambda function handles individual parcel data requests via REST API
with DynamoDB caching, S3 storage, and real-time Snowflake processing.
"""

import json
import boto3
import os
import logging
from datetime import datetime
from typing import Dict, Any, Optional
import sys
sys.path.append('/opt')
from clients.regrid import RegridClient
from utils.simple_snowflake_connector import get_snowflake_connector
from utils.dynamodb_cache import DynamoDBCacheManager

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize clients
regrid_client = RegridClient(api_key='eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJyZWdyaWQuY29tIiwiaWF0IjoxNzU0NTA0ODkyLCJleHAiOjE3ODYwNjE4NDQsImciOjkyMTkzLCJ0IjoxLCJjYXAiOiJwYTp0cyIsInRpIjo2OTIsInRucCI6MX0.AmqtzuOOaCaJ4TSRaR2CUj2_J2ln1Ugyn4iH-IRsSok')
s3_client = boto3.client('s3')
eventbridge = boto3.client('events')
cache_manager = DynamoDBCacheManager(environment=os.environ.get('ENVIRONMENT', 'dev'))

def lambda_handler(event, context):
    """
    API endpoint for individual parcel data requests
    Supports address, coordinates, and parcel ID lookups
    """
    
    try:
        # Parse request
        body = json.loads(event.get('body', '{}'))
        query_type = body.get('type')  # 'address', 'coordinates', 'parcel_id'
        
        # Validate input
        if not query_type:
            return {
                'statusCode': 400,
                'body': json.dumps({'error': 'Missing query type'})
            }
        
        # Generate cache key
        cache_key = generate_cache_key(query_type, body)
        
        # Check DynamoDB cache first
        cached_result = cache_manager.get_cached_parcel(cache_key)
        if cached_result:
            logger.info(f"Cache hit for key: {cache_key}")
            return {
                'statusCode': 200,
                'body': json.dumps(cached_result),
                'headers': {
                    'Content-Type': 'application/json',
                    'X-Cache': 'HIT'
                }
            }
        
        # Fetch from Regrid API
        parcel_data = None
        
        if query_type == 'address':
            address = body.get('address')
            if not address:
                return {
                    'statusCode': 400,
                    'body': json.dumps({'error': 'Missing address parameter'})
                }
            parcel_data = regrid_client.search_by_address(address, limit=1)
            
        elif query_type == 'coordinates':
            lat = body.get('latitude')
            lon = body.get('longitude')
            if not lat or not lon:
                return {
                    'statusCode': 400,
                    'body': json.dumps({'error': 'Missing latitude or longitude'})
                }
            parcel_data = regrid_client.search_by_coordinates(lat, lon)
            
        elif query_type == 'parcel_id':
            parcel_id = body.get('parcel_id')
            if not parcel_id:
                return {
                    'statusCode': 400,
                    'body': json.dumps({'error': 'Missing parcel_id parameter'})
                }
            parcel_data = regrid_client.get_parcel_by_id(parcel_id)
        
        else:
            return {
                'statusCode': 400,
                'body': json.dumps({'error': 'Invalid query type'})
            }
        
        if not parcel_data:
            return {
                'statusCode': 404,
                'body': json.dumps({'error': 'Parcel not found'})
            }
        
        # Prepare response
        response_data = {
            'query': body,
            'result': parcel_data,
            'timestamp': datetime.utcnow().isoformat(),
            'source': 'regrid_api'
        }
        
        # Store to S3 for data lake
        s3_key = f"raw/parcel/api/{datetime.now().strftime('%Y-%m-%d')}/{cache_key}.json"
        s3_client.put_object(
            Bucket='teddy-data-pipeline-bucket-dev',
            Key=s3_key,
            Body=json.dumps(response_data),
            ContentType='application/json',
            Metadata={
                'query_type': query_type,
                'ingestion_type': 'api',
                'cache_key': cache_key
            }
        )
        
        # Load data to Snowflake in real-time
        try:
            with get_snowflake_connector() as snowflake_connector:
                # Test connection first
                test_result = snowflake_connector.test_connection()
                if test_result['status'] == 'success':
                    # For now, just log the successful connection
                    # TODO: Implement proper table insertion once schema is ready
                    logger.info(f"Successfully connected to Snowflake for API data: {cache_key}")
                    logger.info(f"Connection info: {test_result['connection_info']}")
                else:
                    logger.warning(f"Failed to connect to Snowflake: {test_result.get('error', 'Unknown error')}")
        except Exception as sf_error:
            logger.error(f"Snowflake loading error: {str(sf_error)}")
            # Continue processing even if Snowflake fails
        
        # Cache result in DynamoDB (24 hour TTL)
        cache_manager.cache_parcel_data(cache_key, response_data, ttl_hours=24)
        
        # Publish event for real-time processing
        publish_api_event(eventbridge, {
            'bucket': 'teddy-data-pipeline-bucket-dev',
            's3_key': s3_key,
            'query_type': query_type,
            'cache_key': cache_key,
            'ingestion_type': 'api'
        })
        
        return {
            'statusCode': 200,
            'body': json.dumps(response_data),
            'headers': {
                'Content-Type': 'application/json',
                'X-Cache': 'MISS'
            }
        }
        
    except Exception as e:
        logger.error(f"Error in API ingestion: {str(e)}")
        
        # Publish error event
        publish_error_event(eventbridge, {
            'error': str(e),
            'request_body': body,
            'ingestion_type': 'api'
        })
        
        return {
            'statusCode': 500,
            'body': json.dumps({'error': 'Internal server error'})
        }

def generate_cache_key(query_type: str, body: Dict[str, Any]) -> str:
    """Generate DynamoDB cache key based on query parameters"""
    if query_type == 'address':
        return f"parcel:address:{hash(body.get('address', ''))}"
    elif query_type == 'coordinates':
        lat = body.get('latitude')
        lon = body.get('longitude')
        return f"parcel:coords:{lat}:{lon}"
    elif query_type == 'parcel_id':
        return f"parcel:id:{body.get('parcel_id')}"
    else:
        return f"parcel:unknown:{hash(str(body))}"

def publish_api_event(eventbridge, detail):
    """Publish API event for real-time processing"""
    eventbridge.put_events(
        Entries=[
            {
                'Source': 'birddog.parcel-api',
                'DetailType': 'Parcel API Request',
                'Detail': json.dumps(detail)
            }
        ]
    )

def publish_error_event(eventbridge, detail):
    """Publish error event for monitoring"""
    eventbridge.put_events(
        Entries=[
            {
                'Source': 'birddog.parcel-api',
                'DetailType': 'Parcel API Error',
                'Detail': json.dumps(detail)
            }
        ]
    )

def extract_county_from_parcel_data(parcel_data):
    """Extract county from parcel data"""
    if isinstance(parcel_data, list) and len(parcel_data) > 0:
        parcel = parcel_data[0]
    elif isinstance(parcel_data, dict):
        parcel = parcel_data
    else:
        return 'unknown'
    
    # Try various field names for county
    county_fields = ['county', 'county_name', 'admin_county', 'jurisdiction']
    for field in county_fields:
        if field in parcel and parcel[field]:
            return str(parcel[field]).lower()
    
    # Try to extract from address
    if 'address' in parcel and parcel['address']:
        address = str(parcel['address']).lower()
        # Look for common county patterns
        if 'county' in address:
            parts = address.split()
            for i, part in enumerate(parts):
                if part == 'county' and i > 0:
                    return parts[i-1]
    
    return 'unknown'

def extract_state_from_parcel_data(parcel_data):
    """Extract state from parcel data"""
    if isinstance(parcel_data, list) and len(parcel_data) > 0:
        parcel = parcel_data[0]
    elif isinstance(parcel_data, dict):
        parcel = parcel_data
    else:
        return 'unknown'
    
    # Try various field names for state
    state_fields = ['state', 'state_code', 'admin_state', 'region']
    for field in state_fields:
        if field in parcel and parcel[field]:
            state_value = str(parcel[field]).lower()
            # Convert state codes to full names
            if state_value == 'tx':
                return 'texas'
            elif state_value == 'ca':
                return 'california'
            elif state_value == 'fl':
                return 'florida'
            else:
                return state_value
    
    # Try to extract from address
    if 'address' in parcel and parcel['address']:
        address = str(parcel['address']).lower()
        if 'tx' in address or 'texas' in address:
            return 'texas'
        elif 'ca' in address or 'california' in address:
            return 'california'
        elif 'fl' in address or 'florida' in address:
            return 'florida'
    
    return 'unknown'
